这里是中文的readme.

该软件模拟程序分成两个主要部分，一部分是软件交换机模型模拟，一部分是网络拓扑结构搭建。
这里我们设计了三个use case。分别命名为mpls，ecmp，和multi-tenant。


注意以下几个问题：

1. 本项目全部构建于Ubuntu 18.04操作系统，未对其他系统做运行验证。

2. 在运行该项目之前，请务必检查自己的电脑上是否存在mininet，grpc，protobuf，等一系列需要的依赖。

3. 在构建之前务必检查本项目的CMakeLists 和您的服务器配置是否匹配，注意根据自己的机器合理配置您的Cmake。

4. 本项目由于输出众多，可能需要开启多个窗口，请注意留足至少四个窗口用于观察输出。

5. 两个模块虽然独立，但是在路径上walker_mininet高度耦合sw_dataplane，两个模块可以作为整体移动，但是除非十分了解代码中哪些位置引用了dataplane，否则建议不要随意移动代码位置。


运行步骤：

1. 打开mininet构建网络。

进入walker_mininet。
cd walker_mininet

然后选择一个use case进行运行，这里首先运行mpls。

指令对应的是
make mininet uc=mpls

之后的use case只需调整uc=xxx即可。

命令结束，该窗口就是用于各个主机互ping以及观察拓扑结构的窗口。

2. 打开一个新窗口，建立data plane

依旧进入walker_mininet找到Makefile

cd walker_mininet
make data_plane uc=mpls

命令结束，该窗口就是用于创建交换机并观察交换机的输出的窗口，交换机的输出正常情况下应该是包走过的交换机路径。
该路径是多台交换机共同输出的结果，而并非一台交换机。但在配置下发之前，交换机处于静默状态。

3. 再打开一个窗口，建立control plane

这次的命令依旧使用Makefile

cd walker_mininet
make controller uc=mpls

该窗口即用于查看控制器的输出，控制器输出了一些算法和分配的具体情况。

4. 再打开一个窗口，作为实际的对交换机进行初始化和更新的操作窗口。

cd walker_mininet
make init uc=mpls

该动作实现对本个样例全部的交换机进行配置和流表的初始化。正常情况下， 控制器，数据平面都会显示操作输出，如果mininet在ping的话，正常也全部能够ping通，而且会在数据平面窗口输出每个ping包走过的路径信息，如果没有正确的话，例如段错误，或者无反应，请检查下发的配置是否正确，如果是段错误，说明下发的配置是错误的，如果是无反应，说明未下发配置，也许是控制器故障，也许是文件路径有问题。

5. 这回不需要再打开一个新的窗口了，待输出稳定后，即可直接输入接下来的更新指令

make update uc=mpls

该动作实现对本个样例所有的交换机的配置和流表进行更新，正常情况下，控制器，数据平面都会显示输出，而且如果在ping的话，数据平面的行为会发生变化，具体变化过程参考输出日志。


对于另外两个use case：
ecmp的use case只需要把每个make指令后的mpls换成ecmp即可。

multi_tenant的特殊一些，multi_tenant的发包不是通过ping完成的，而是通过make指令调用python脚本来完成的
make send1 uc=multi_tenant，即代表主机1需要发包，相应的还有：
make send2 uc=multi_tenant
make send3 uc=multi_tenant


代码从接口上的描述，描述一下代码之间的依赖关系：

对于dataplane的代码需要接受一个记录了各个端口interface的yaml文件，确定该交换机安装在了网络的哪个节点，另外还需要一个grpc的端口来作为接受controller配置和流表的端口。yaml文件位于walker_mininet的interface中，启动一台虚拟机时直接将对应yaml文件路径作为参数传入程序。

控制器下发的流表的json在walker_mininet的entry文件夹中，这些json是通过代码生成的，并非手写，代码文件在walker_mininet/walker_mininet/<use case>/<use case>_build_entry.py中，而且最终传入数据平面的也不是json，写json是为了方便。
这段代码除了各种数字地址不太好理解以外，文件结构还是很好懂的，如果发现流表有问题，估计就是里边给的值有问题。

控制器下发配置的json应该是编译生成的，现在rp4-new代码的路径那个root。。现在是path。那段代码应该给path改成那个temp路径重新编译才能跑，但是我不知道该怎么编译那段代码，让张寅超重新编译吧如果需要的话。
他是这样，控制器每一秒钟就扫描那个文件夹，看有没有python脚本送进去的文件，如果有就启动控制器编译，然后下发到dataplane上去，如果没有就继续扫。

对于mininet，他是通过topology文件建立拓扑网络，文件位于topology文件夹中。不同的use case调用了不同的拓扑文件，而且分成更新前和更新后两个文件，两个文件的内容大约只有流表有区别。

对于日志，如果输出除控制台输出以外的日志的话，都应该在logs中。

简单介绍一下我在这个交换机里写的代码：

这里一个东西就是BitArray类型，他实际就相当于一个可变长度的bitset。可以通过不同的构造函数构造出不同功能的网络序的比特数组，测试代码当中应该有对他的应用案例。他可以通过一个立即数构造，也可以通过一个网络包构造（当然后来发现这个问题的确是我多虑了，但写都写了，就没再继续思考怎么给他改回来的问题）。

然后是四层和pipeline，这个就是基于以前的代码加以改造形成的，没啥。

然后就是main函数了，main函数的代码不少内容是专门为了演示写的，花花绿绿的输出。


对代码改进的建议：

我觉得这部分代码至少还有以下几个改进办法：

一个是给物理层写成阻塞的，每开一个交换机cpu就跑满一个，我觉得这么弄做不了一些规模比较大的模拟，浪费资源还低效，如果设计成阻塞的物理层，那一台服务器可以支持几百个这样的交换机模拟而不至于死机，具体的代码我写了一个WalkerNet在github上，不知道能不能找到，大体思路就是给每一个端口都开启一个线程，然后设计一个线程安全的队列，每个线程接收到包就送进队列中去，其他线程需要的时候就从队列中获取一个包，获取不到就阻塞，直到能获取为止，实际就是一个生产者消费者模型。本来是实习之后打算自己继续弄一些网络实验的，看来是没时间弄了，他现在只有一个物理层地基的代码。

一个是控制器那部分有时间应该做成基于socket的同步机制应该要更好一点，一秒一秒扫，再慢也是忙等，不太美观。

一个是对于mininet，当时是时间紧迫没时间，其实交换机应该是确确实实能做进mininet的，但是一没时间学二没时间研究，能做啥样是啥样了，不过有时间还是应该研究一下如何优雅的在启动mininet的时候就给交换机也一并启动而且还能看到输出。

那个bitarray想用就用吧，那个代码现在看来还是有瑕疵。








